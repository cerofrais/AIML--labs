{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment2-Lab03.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"ocOYueAj0IKZ","colab_type":"text"},"cell_type":"markdown","source":["# Session 3\n","## Experiment 2\n","###  Lab"]},{"metadata":{"id":"ntDDF43v0IKh","colab_type":"text"},"cell_type":"markdown","source":["In this lab we will use WINE dataset\n","\n","### Data Source\n","https://archive.ics.uci.edu/ml/datasets/wine\n","\n","### Objective\n","To understand Scaling and Normalization\n","\n","#### Dataset Information:\n","These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. \n","\n","### Data Attributes\n","\n","1. Alcohol \n","2. Malic acid \n","3. Ash \n","4. Alcalinity of ash \n","5. Magnesium \n","6. Total phenols \n","7. Flavanoids \n","8. Nonflavanoid phenols \n","9. Proanthocyanins \n","10. Color Intensity\n","11. Hue\n","12. OD280/OD315 of diluted wines \n","13. Proline \n","\n","\n","\n","### Predicted attribute\n","The first field in the data is the Class Label -- 1-3"]},{"metadata":{"id":"gWWd7gK60IKt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import pandas as pd\n","data = pd.read_csv(\"wine_data.csv\", header=None)\n","#print(data.shape)\n","print(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Pe0itpvu0ILK","colab_type":"text"},"cell_type":"markdown","source":["### Extract the features Alcohol (percent/volume) and Malic acid (g/l). "]},{"metadata":{"id":"eDUwYZei0ILP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["## Code here\n","features_extracted = data[[0,1,2]].values\n","#print features_extracted"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dS6_sCdn0ILe","colab_type":"text"},"cell_type":"markdown","source":["###  Plot a graph between Alcohol (percent/volume) and Malic acid (g/l). \n","#### Can you see some sparsity and non-symmetry in the dataset?"]},{"metadata":{"id":"EijlNU4K0ILi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["## Your code here\n","import matplotlib.pyplot as plt\n","plt.figure(1, figsize=(20,10))\n","plt.scatter(features_extracted[:,1], features_extracted[:,2], c=features_extracted[:,0],s=60)\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pVq6UE4T0ILx","colab_type":"text"},"cell_type":"markdown","source":["## Min-Max Scaling\n","Min-Max scaling  maps the features in the range of [0, 1]. The formula for min-max scaling is below:\n","\n","\\begin{equation*}\n","    x_{norm}=\\frac{x-x_{min}}{x_{max} - x_{min}}\n","\\end{equation*}\n","\n","Let us scale the Alcohol and Malic Acid columns. Plot the graph using these scaled values. Do you see any difference?"]},{"metadata":{"id":"8h05L4ds0IL3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","\n","## Min-max normalization\n","def xnorm(x, xmin, xmax):\n","    return (x - xmin)/(xmax - xmin) \n","def minmax(dataSet):\n","    xmin = np.min(dataSet,0)\n","    xmax = np.max(dataSet,0)\n","    return xnorm(dataSet,xmin,xmax)\n","\n","\n","scaled_features = minmax(features_extracted[:,1:])\n","print(scaled_features.shape)\n","\n","### Only Scaled Plot ### \n","plt.figure(1, figsize=(20,10))\n","plt.scatter(scaled_features[:,0], scaled_features[:,1], c=features_extracted[:,0],s=60)\n","plt.show()\n","\n","### Raw Data Vs Scaled Data Plot ### Observe the values become very small after scaling\n","plt.figure(1, figsize=(20,10))\n","plt.scatter(features_extracted[:,1], features_extracted[:,2], c='b',s=60)\n","plt.scatter(scaled_features[:,0], scaled_features[:,1], c='r',s=60)\n","plt.show()\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Oniw_yXL0IMB","colab_type":"text"},"cell_type":"markdown","source":["## Standardization Method\n","Given $x$ is the original data, $\\mu$ is the mean of a particular feature and $\\sigma$ is the standard deviation scale the features. The formula for feature Normalization is:\n","    \\begin{equation*}\n","          x_{norm}=\\frac{x-\\mu}{\\sigma}\n","   \\end{equation*}\n","   \n","**Exercise 4** :: Plot a graph between Alcohol (percent/volumne) and Malic acid (g/l). Do you see some difference between this and min-max plotting and  plotting raw data?\n"]},{"metadata":{"id":"FHpwGrOM0IMI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","def xZscore(x, mu, sigma):\n","    return (x - mu)/sigma \n","def zScore(dataSet):\n","    avg = np.mean(dataSet,0)\n","    std = np.std(dataSet,0)\n","    return xZscore(dataSet, avg, std) \n","\n","\n","std_features = zScore(features_extracted[:,1:])\n","print(std_features.shape)\n","\n","plt.figure(1, figsize=(20,10))\n","### Plot1: Plot Standardised features\n","plt.scatter(std_features[:,0], std_features[:,1], c=features_extracted[:,0],s=60)\n","plt.show()\n","\n","### Plot2: Standard Data Vs Raw Data Plot \n","### Observe that the values become very small after scaling\n","plt.figure(1, figsize=(20,10))\n","plt.scatter(features_extracted[:,1], features_extracted[:,2], c='b',s=60)\n","plt.scatter(std_features[:,0], std_features[:,1], c='r',s=40)\n","plt.show()\n","\n","### Plot3: Scaled data Vs standard Data Vs Raw data Plot \n","### Observe the values become very small after scaling\n","plt.figure(1, figsize=(20,10))\n","plt.scatter(scaled_features[:,0], scaled_features[:,1], c='r',s=60)\n","plt.scatter(features_extracted[:,1], features_extracted[:,2], c='b',s=60)\n","plt.scatter(std_features[:,0], std_features[:,1], c='y',s=60)\n","plt.show()\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-MGskA4y0IMW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### Intuition behind differnce in standardization and scaling ####\n","\n","x = range(10)\n","y = range(10)\n","\n","plt.figure(1, figsize=(20,10))\n","plt.plot(x, y, \"ko\", ms = 10)\n","plt.plot(x, minmax(y), \"ro\", ms = 10)\n","plt.plot(x, zScore(y), \"bo\", ms = 10)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g1N6X6tF0IMm","colab_type":"text"},"cell_type":"markdown","source":["## Exercises\n","Try the above with the following y values\n","  * y = range(10, 20)\n","  * y = range(20, 10, -1)\n","  * y = range(50, 100, 5)"]},{"metadata":{"id":"hnCBkPkF0IMs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["y = range(10, 20)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A48A8hv70IM3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["y = range(20, 10, -1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"byYcoazx0INL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["y = range(50, 100, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9G15r9BB0INY","colab_type":"text"},"cell_type":"markdown","source":["## Classification\n","Comparison of classification results using raw features, scaled features and standardised features. You can use any classifier  like KNN, Linear or Naive Bayes"]},{"metadata":{"id":"LTjECr6c0INa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### Lets compare the scaled features and normalized in a task of classification \n","### We will use the wine data set in a KNN classifier and see the accuracies.\n","\n","#import packages\n","import math\n","import collections\n","import random\n","\n","\n","# ------------------------------------------------ #\n","# We are assuming that the label is the last field #\n","# If not, munge the data to make it so!            #\n","# ------------------------------------------------ #\n","\n","def dist(a, b):\n","    sqSum = 0\n","    for i in range(len(a)):\n","        sqSum += (a[i] - b[i]) ** 2\n","    return math.sqrt(sqSum)\n","\n","def kNN(k, train, given):\n","    distances = []\n","    for t in train:\n","        distances.append((dist(t[:-1], given[:-1]), t[-1]))\n","    distances.sort()\n","    return distances[:k]\n","\n","def kNN_classify(k, train, given):\n","    tally = collections.Counter()\n","    for nn in kNN(k, train, given):\n","        tally.update(str(int(nn[-1])))\n","    return tally.most_common(1)[0]\n","\n","picker = list(range(data.shape[0]))\n","random.shuffle(picker)       \n","\n","FEATURE_COLUMNS = list(range(1, 14))\n","ALL_COLUMNS = FEATURE_COLUMNS + [0]\n","\n","TRAIN_TEST_RATIO = 0.8\n","\n","## Raw Data ###\n","data = data.reindex(columns = ALL_COLUMNS)\n","trainMax = int(len(picker) * TRAIN_TEST_RATIO)\n","train = []\n","test = []\n","for pick in picker[:trainMax]:\n","    train.append(list(data.values[pick]))         ### select 80% of data to be used as training set\n","for pick in picker[trainMax:]:\n","    test.append(list(data.values[pick])) \n","\n","acc = []\n","for t in test:\n","     acc.append(str(int(t[-1])) == kNN_classify(5, train, t)[0])\n","\n","print(\"Accuracy without any normalization: \", sum(acc)/(len(test)*1.0))\n","\n","## Scaled data ###\n","scaled_feats = minmax(data[FEATURE_COLUMNS].values)\n","scaled_data = np.append(scaled_feats, data[0].values.reshape(data.shape[0],1),1)\n","\n","train = []\n","test = []\n","\n","for pick in picker[:trainMax]:\n","    train.append(list(scaled_data[pick]))         ### select 80% of data to be used as training set\n","for pick in picker[trainMax:]:\n","    test.append(list(scaled_data[pick])) \n","\n","acc = []\n","for t in test:\n","     acc.append(str(int(t[-1])) == kNN_classify(5, train, t)[0])\n","\n","print(\"Accuracy with scaling: \", sum(acc)/(len(test)*1.0))\n","\n","### Standardized Data ###\n","\n","std_feats = zScore(data[FEATURE_COLUMNS].values)\n","std_data = np.append(std_feats, data[0].values.reshape(data.shape[0],1),1)\n","\n","train = []\n","test = []\n","for pick in picker[:trainMax]:\n","    train.append(list(std_data[pick]))         ### select 80% of data to be used as training set\n","for pick in picker[trainMax:]:\n","    test.append(list(std_data[pick])) \n","\n","acc = []\n","for t in test:\n","     acc.append(str(int(t[-1])) == kNN_classify(5, train, t)[0])\n","\n","print(\"Accuracy with standardization:\", sum(acc)/(len(test)*1.0))"],"execution_count":0,"outputs":[]}]}