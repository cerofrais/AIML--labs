{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment1-Lab1.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"4xbq4pR6b5mf","colab_type":"text"},"cell_type":"markdown","source":["# Session 1\n","## Experiment 1\n","### Lab"]},{"metadata":{"id":"tlu6j0eab5mh","colab_type":"text"},"cell_type":"markdown","source":["In this experiment, we will use the data set on fruits which we explored earlier and learn how a simple K nearest neighbour classification works. "]},{"metadata":{"id":"hvYtjbP4b5mi","colab_type":"text"},"cell_type":"markdown","source":["Let us consider a simple situation. Given some data about a fruit, we want to label it automatically.\n","\n","Fruits are characterized by \n"," * weight in grams as a float\n"," * colour as an integer\n","     - 1 $\\rightarrow$ red\n","     - 2 $\\rightarrow$ orange\n","     - 3 $\\rightarrow$ yellow\n","     - 4 $\\rightarrow$ green\n","     - 5 $\\rightarrow$ blue\n","     - 6 $\\rightarrow$ purple\n"," * label as a string\n","     - \"A\" $\\rightarrow$ Apple\n","     - \"B\" $\\rightarrow$ Banana\n","     \n","We are given some sample data such as (303, 3, \"A\") meaning the fruit with 303 gram weight, and yellow colour is an apple. A set of such *training examples* is given in “01-train.csv”. This has a small set of 17 **labeled** examples. \n","\n","We are given a set of **test** data where only weight and colour are given,  eg. (373,1). We should design a simple Nearest Neighbour classifier that will find the fruit label. i.e., \"A\" or \"B\", meaning Apple or Banana. \n","\n","We have 102 such testcases. We are also given additional files which have the correct labels for all the 102 test cases. If your predicted label is correct, you have done well!\n","\n","Here are the details of all the files:\n","  * 01-train.csv $\\Rightarrow$ The original input data. \n","    - 18 lines\n","    - the first line is a header\n","    - each of the remaining 17 lines has three pieces of data:\n","       * weight in grams :: float\n","       * colour code :: 1, 2, 3, 4, 5 \n","       * label :: \"A\", \"B\"\n","  * 01-test1.csv $\\Rightarrow$ The first test data set.\n","    - 31 lines\n","    - the first line is a header\n","    - each of the remaining 30 lines has two pieces of data\n","       * weight in grams :: float\n","       * colour code :: 1, 2, 3, 4, 5\n","  * 01-test1-labels.csv $\\Rightarrow$ The labels for test data set above. That is, each line has just the correct label.\n","  * 01-test1-labelled.csv $\\Rightarrow$ The above two files combined. \n","  * 01-test2.csv $\\Rightarrow$ The second test data set. Similar to the first data set, except that it has 73 lines.\n","  * 01-test2-labels.csv $\\Rightarrow$ The labels for test data set above. That is, each line has just the correct label.\n","  * 01-test2-labelled.csv $\\Rightarrow$ The above two files combined. "]},{"metadata":{"id":"4P-xfeEvb5mi","colab_type":"text"},"cell_type":"markdown","source":["We  see that similar fruits come close in the feature (weight, color) space? Now let us plot one sample data given in black."]},{"metadata":{"id":"KkcWffn3b5mj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Let us first read the data from the file and do a quick visualization\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","train = pd.read_csv(\"../Datasets/01-train.csv\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5gtX_rX3b5mm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["apples = train[train.Label == \"A\"]\n","bananas = train[train.Label == \"B\"]\n","plt.plot(apples.Weight, apples.Colour, \"ro\")\n","plt.plot(bananas.Weight, bananas.Colour, \"y+\")\n","plt.xlabel(\"Weight -- in grams\")\n","plt.ylabel(\"Colour -- r-o-y-g-b-p\")\n","plt.legend([\"Apples\", \"Bananas\"])\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KmYbuwxfb5mo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.plot(apples.Weight, apples.Colour, \"ro\")\n","plt.plot(bananas.Weight, bananas.Colour, \"y+\")\n","plt.xlabel(\"Weight -- in grams\")\n","plt.ylabel(\"Colour -- r-o-y-g-b-p\")\n","plt.legend([\"Apples\", \"Bananas\"])\n","plt.plot([373], [1], \"ko\")\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xi7u1hNEb5mq","colab_type":"text"},"cell_type":"markdown","source":["From the visualization alone, we can infer that the unknown fruit is likely to be an apple. \n","\n","The job now is to instead of eyeballing it one at a time like above, use a kNN classifier with, say, $k = 3$ and using the *Euclidean* distance, to determine the correct label for the data in the file \"01-test1.csv\" that has 30 data points. \n","\n","Let us first write a distance function to calculate the *Euclidean* distance between two fruits."]},{"metadata":{"id":"CY4PcDXCb5mr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import math\n","def dist(a, b):\n","    sqSum = 0\n","    for i in range(len(a)):\n","        sqSum += (a[i] - b[i]) ** 2\n","    return math.sqrt(sqSum)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KbNzPMs3b5ms","colab_type":"text"},"cell_type":"markdown","source":["\n","Now let us write code to find the $k$ nearest neighbours of a given fruit"]},{"metadata":{"id":"bVCF1c2Xb5mt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def kNN(k, train, given):\n","    distances = []\n","    for t in train.values:              \n","        # loop over all training samples\n","        distances.append((dist(t[:2], given), t[2])) \n","        # compute and store distances with respect to each training sample\n","    distances.sort()            \n","    return distances[:k]    # return first k samples = nearest  k distances to the given sample"],"execution_count":0,"outputs":[]},{"metadata":{"id":"92apL7kCb5mx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(kNN(5, train, (340, 1)))\n","print(kNN(5, train, (373, 1)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qcAk2l-mb5m2","colab_type":"text"},"cell_type":"markdown","source":["As you can see above, the 3 (and 5) nearest neighbours of the fruit with the characteristics (373, 1) are all Apples -- label 1; which is what we visually saw when we plotted the point as a black spot in the chart. Of course we need to write another function to get this attribute rather than read, so we have written a function for that. We have used collections.Counter, which is a very useful python library. More detail are at:\n","\n"," * https://docs.python.org/3/library/collections.html#collections.Counter "]},{"metadata":{"id":"ltLUvtbab5m3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import collections\n","def kNNmax(k, train, given):\n","    tally = collections.Counter()\n","    for nn in kNN(k, train, given):\n","        tally.update(nn[-1])\n","    return tally.most_common(1)[0]\n","print(kNNmax(5, train, (340, 1)))\n","print(kNNmax(7, train, (340, 1)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RKhhNCB6b5m7","colab_type":"text"},"cell_type":"markdown","source":["This shows that of the five nearest neighbours to (340, 1) four are Apples and of the seven nearest, five are Apples "]},{"metadata":{"id":"55rW2tIEb5m7","colab_type":"text"},"cell_type":"markdown","source":["Now let us load the test data and find the labels for all of them "]},{"metadata":{"id":"vEJBKlHQb5m8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["testData1 = pd.read_csv('../Datasets/01-test1.csv').values\n","test1labels = pd.read_csv('../Datasets/01-test1-labels.csv').values\n","for t,t1 in zip(testData1,test1labels):\n","    print(t, kNNmax(1, train, t)[0], t1)\n","    \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QINeCabTb5m_","colab_type":"text"},"cell_type":"markdown","source":["Let us count how many are correct, instead of displaying the results"]},{"metadata":{"id":"xTNJDC_sb5nA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["testData = pd.read_csv('../Datasets/01-test1.csv').values\n","testResults = pd.read_csv('../Datasets/01-test1-labels.csv').values.flatten()\n","results = []\n","    \n","for i, t in enumerate(testData):\n","    results.append(kNNmax(3, train, t)[0] == testResults[i])\n","\n","print(results.count(True), \"are correct\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CaMWRwINb5nC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["len(testData)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oQaU8mndb5nI","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 1** :: Find the accuracy of your prediction -- percentage of the samples that are correctly predicted."]},{"metadata":{"id":"KTnyuT-ib5nI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(\"Accuracy is \",results.count(True)/len(testData) * 100 , \"%\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1oCY4vtIb5nK","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 2** :: Predict the labels for the larger file \"01-test2.csv\" that has 72 data points\n"]},{"metadata":{"id":"Pv0EDjZEb5nK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["testData2 = pd.read_csv('../Datasets/01-test2.csv').values\n","for t in testData2:\n","    print(t,kNNmax(3,train,t)[0])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VVQ59mGUb5nM","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 3** :: Find the accuracy of the prediction by comparing with \"01-test2-labelled.csv\" "]},{"metadata":{"id":"wskKxFCzb5nM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["testResults2 = pd.read_csv('../Datasets/01-test2-labels.csv').values.flatten()\n","results2 = []\n","for i,t in enumerate(testData2):\n","    results2.append(kNNmax(3,train,t)[0] == testResults2[i])\n","print(results2.count(True), \" are correct\")\n","print(\"Accuracy is \",results2.count(True)/len(testData2)*100, \" %\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RFzEZUuhb5nO","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 4** :: Repeat the above experiment with $k = 5$ and $k = 7$. Explain which $k$ is better and why?\n","\n","**Exercise 5** :: Repeat the above experiment with $k = 17$. What do you think is happening?\n"]},{"metadata":{"id":"swJQzsRQb5nO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def accuracy(train,test,testResults,k):\n","    results = []\n","    for i,t in enumerate(test):\n","        results.append(kNNmax(k,train,t)[0] == testResults[i])\n","        \n","    print(results.count(True), \" are correct\")\n","    print(\"Accuracy is \",results.count(True)/len(test)*100, \" %\")\n","    \n","    \n","accuracy(train,testData2,testResults2,5)\n","accuracy(train,testData2,testResults2,7)\n","accuracy(train,testData2,testResults2,17)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"prTw01Cgb5nQ","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 6** :: If the weights are in Kgs, that is divide all of the data in weights column by 1000, what is the accuracy for $k = 3$\n"]},{"metadata":{"id":"SyjUQbKqb5nQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["TraininKG = pd.read_csv(\"../Datasets/01-train.csv\")\n","TraininKG['Weight'] = TraininKG['Weight']/1000\n","TestData2inKG = pd.read_csv('../Datasets/01-test2.csv')\n","TestData2inKG['Weight'] = TestData2inKG['Weight']/1000\n","TestData2inKG = TestData2inKG.values"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2augud9Xb5na","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["accuracy(TraininKG,TestData2inKG,testResults2,3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ec07FEC9b5nc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### Calculation of weight can be done using pandas converts function\n","\n","def convert_weight(weight):\n","    return int(weight)/1000\n","\n","def convert_weights(weight):\n","    return float(weight)/1000\n","\n","Traininkgs = pd.read_csv(\"../Datasets/01-train.csv\", converters={\"Weight\":convert_weight})\n","Testdata2inkgs = pd.read_csv(\"../Datasets/01-test2.csv\", converters={\"Weight\":convert_weights})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YLYs_S4ub5nd","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 7** :: Modify the distance function to ignore the colour feature. Calculate the accuracy on \"01-test1.csv\"\n"]},{"metadata":{"id":"Yf8y2nlZb5ne","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import math\n","def dist(a, b):\n","    sqSum = 0\n","    for i in range(len(a)-1):\n","        sqSum += (a[i] - b[i]) ** 2\n","    return math.sqrt(sqSum)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LBRUj2HNb5nf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["testData = pd.read_csv('../Datasets/01-test1.csv').values\n","testResults = pd.read_csv('../Datasets/01-test1-labels.csv').values.flatten()\n","results = []\n","    \n","for i, t in enumerate(testData):\n","    results.append(kNNmax(3, train, t)[0] == testResults[i])\n","\n","print(results.count(True), \"are correct\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7LaqlaFIb5ng","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["results.count(True)/len(testData) * 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xKhF5Rp4b5ni","colab_type":"text"},"cell_type":"markdown","source":["#### Result: Accuracy is 100% for all test data"]},{"metadata":{"id":"ypNleRsbb5ni","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 8** :: If we used the square of the Euclidean distance, for the distance function does it affect the accuracy?"]},{"metadata":{"id":"xVYTdareb5nj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import math\n","def dist(a, b):\n","    sqSum = 0\n","    for i in range(len(a)-1):\n","        sqSum += (a[i] - b[i]) ** 2\n","    return sqSum"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CwJkFlGtb5nl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["testData = pd.read_csv('../Datasets/01-test1.csv').values\n","testResults = pd.read_csv('../Datasets/01-test1-labels.csv').values.flatten()\n","results = []\n","    \n","for i, t in enumerate(testData):\n","    results.append(kNNmax(3, train, t)[0] == testResults[i])\n","\n","print(results.count(True), \"are correct\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lMFwS-v1b5np","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["results.count(True)/len(testData) * 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PfV2iJd_b5ns","colab_type":"text"},"cell_type":"markdown","source":["#### Result : No effect"]},{"metadata":{"id":"BOlRLLKCb5nt","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 9** :: If we use the sum of the absolute differences, as the distance metric instead of the Euclidean, how does that affect the accuracy?\n","\n","**Result:** Slight change in Accuracy if color feature is included in dist function"]},{"metadata":{"id":"EnC6e0GFb5nt","colab_type":"text"},"cell_type":"markdown","source":["## Acknowledgment\n","This experiment is based on the blog post http://www.jiaaro.com/KNN-for-humans. "]},{"metadata":{"id":"Kgh6Hz77b5nv","colab_type":"text"},"cell_type":"markdown","source":["## Summary\n","In the above experiment, we find that a simple nearest neighbour method can successfully predict labels with a small number of labelled examples. But we also see that the results can go really wrong if we make some wrong choices (like weight in Kg, or a very large K). This should remind you about the practical expertise and experimental skills that will become equally important as we move forward."]}]}