{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab04_Experiment3.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"6-4Qd-uS1vbH","colab_type":"text"},"cell_type":"markdown","source":["# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n","# Lab04 Experiment 3"]},{"metadata":{"id":"wKIUolYi1vbI","colab_type":"text"},"cell_type":"markdown","source":["# SPEECH RECOGNITION"]},{"metadata":{"id":"pSCZFZlX1vbJ","colab_type":"text"},"cell_type":"markdown","source":["Speech has been one of the most widely used mode of communication. One of the part of our day-today communication is to give and receive commands/instructions. Making a machine understand these instructions to perform certain activities can be a boon for a host of applications.\n","\n","In this experiment we will work on recognizing speech into 30 commands.\n","\n","- Firstly, we will explore and visualize audio features for speech.\n","\n","\n","- We will then use these features to train different classifiers, to classify them into 30 different classes:\n","    - kNN classifier with MFCC features\n","    - kNN classifier with Deep features\n","    - Naive Bayes classifier with MFCC features\n","    - Naive Bayes classifier with Deep features\n","    - Multi-class linear classifier with MFCC features\n","    - Multi-class linear classifier with Deep features\n","\n","\n","- Lastly, we will use different evaluation measures to compare the performances of our classifiers."]},{"metadata":{"id":"OVn3Acm51vbK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":619},"outputId":"960a708e-5e71-4b35-a237-fc51b1c05f99","executionInfo":{"status":"ok","timestamp":1527160672849,"user_tz":-330,"elapsed":12474,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"104611679557974953873"}}},"cell_type":"code","source":["!pip install librosa      #required library for experiment\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting librosa\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/f4/422bfbefd581f74354ef05176aa48558c548243c87e359d91512d4b65523/librosa-0.6.0.tar.gz (1.5MB)\n","\u001b[K    100% |████████████████████████████████| 1.5MB 11.6MB/s \n","\u001b[?25hCollecting audioread>=2.0.0 (from librosa)\n","  Downloading https://files.pythonhosted.org/packages/21/81/d6dfb88379ab03d7f385c5a2e5ed045af0959418c38ce7cc4efc0fba0da2/audioread-2.1.5.tar.gz\n","Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.14.3)\n","Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.19.1)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.19.1)\n","Collecting joblib>=0.7.0 (from librosa)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/870b2ec270fc29c5d89f85353da420606a9cb39fba4747127e7c7d7eb25d/joblib-0.11-py2.py3-none-any.whl (176kB)\n","\u001b[K    100% |████████████████████████████████| 184kB 5.1MB/s \n","\u001b[?25hRequirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.3.0)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n","Collecting resampy>=0.2.0 (from librosa)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/85/275f54cca4ae05a70488016af5a833b5361abb53dd5c5f4c0a7af8d50c4c/resampy-0.2.0.tar.gz (322kB)\n","\u001b[K    100% |████████████████████████████████| 327kB 11.5MB/s \n","\u001b[?25hCollecting numba>=0.32 (from resampy>=0.2.0->librosa)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/36/41d894e24e0d00d85e7ed10880deb391f6b6679ae43db25ea6d036476c8c/numba-0.38.0-cp36-cp36m-manylinux1_x86_64.whl (1.9MB)\n","\u001b[K    100% |████████████████████████████████| 1.9MB 13.1MB/s \n","\u001b[?25hCollecting llvmlite>=0.23.0dev0 (from numba>=0.32->resampy>=0.2.0->librosa)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/71/9ac956af0bc8e6624db7ff1653426a4d3f2fcb72bc6707f022158e62b64f/llvmlite-0.23.0-cp36-cp36m-manylinux1_x86_64.whl (15.8MB)\n","\u001b[K    100% |████████████████████████████████| 15.8MB 1.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: librosa, audioread, resampy\n","  Running setup.py bdist_wheel for librosa ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/0d/19/fa/71097e2207df1cc613749f15b2f0b1972c167b36d6afc09d15\n","  Running setup.py bdist_wheel for audioread ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/6d/68/77/4dbeecc26ebdd91b42bfd7a5c50f23f1356893559758049787\n","  Running setup.py bdist_wheel for resampy ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/c7/52/ba/03ae677b7daeda1cfacd878f19d095f9bc81c408be6d634709\n","Successfully built librosa audioread resampy\n","Installing collected packages: audioread, joblib, llvmlite, numba, resampy, librosa\n","Successfully installed audioread-2.1.5 joblib-0.11 librosa-0.6.0 llvmlite-0.23.0 numba-0.38.0 resampy-0.2.0\n"],"name":"stdout"}]},{"metadata":{"id":"FUThiZwi1vbO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":353},"outputId":"6dcb839d-f9cf-4a89-83cf-54147f96549a","executionInfo":{"status":"error","timestamp":1527160674481,"user_tz":-330,"elapsed":1585,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"104611679557974953873"}}},"cell_type":"code","source":["import os\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Math\n","import numpy as np\n","from scipy import signal\n","from scipy.io import wavfile\n","import librosa\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import librosa.display\n","\n","from matplotlib.pyplot import *\n","%matplotlib inline\n","\n","from classifiers import *"],"execution_count":3,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-5e2d9c9ce854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclassifiers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'classifiers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"metadata":{"id":"kXPJHrHJ1vbR","colab_type":"text"},"cell_type":"markdown","source":["# Dataset\n","\n","For this exercise we will use TensorFlow’s Speech Commands Datasets which includes 65,000 one second-long utterances of 30 short commands. This dataset has been curated using thousands of people and is opensource under a Creative Commons BY 4.0 license.\n","\n","Example commands: 'Yes', 'No', 'Up', 'Down', 'Left', etc."]},{"metadata":{"id":"oXAyCbuY1vbT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["unique_classes = np.array(['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy',\n","                           'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven',\n","                           'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h_7RC4Aw1vbV","colab_type":"text"},"cell_type":"markdown","source":["Let us read a sample audio file from this dataset: "]},{"metadata":{"id":"lzaa-gdA1vbW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["audio_file_path = '../Datasets'\n","filename = '/5e1b34a6_nohash_0.wav'\n","sample_rate, samples = wavfile.read(str(audio_file_path) + filename)\n","print(sample_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mhzNiIwH1vbZ","colab_type":"text"},"cell_type":"markdown","source":["# 1. MFCC features"]},{"metadata":{"id":"5p2jlXyf1vba","colab_type":"text"},"cell_type":"markdown","source":["##  1.1. Amplitude\n","\n","Speech is a temporal signal, where the amplitude of the signal varies with time.\n","\n","The amplitude v/s time graph of the audio file we read is:"]},{"metadata":{"id":"NvuGs-K51vba","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["fig = plt.figure(figsize=(14, 8))\n","plt.plot(np.linspace(0, sample_rate/len(samples), sample_rate), samples)\n","plt.gca().set_title('Raw wave of ' + filename)\n","plt.gca().set_ylabel('Amplitude')\n","plt.grid(\"on\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q1j0_7FY1vbd","colab_type":"text"},"cell_type":"markdown","source":["## 1.2 Log Spectrogram\n","\n","The same speech signal could be interpreted as made up of several frequencies of waves. A visualization of the power, i.e. energy per unit time in each frequency v/s time is called the Spectrogram.\n","\n","Usually, the log of this energy is considered a better parameter. This is because the power in sound is synonymous with volume (loudness) of the sound, and the human ears are more sensitive to smaller volumes than larger volumes. So it is more convenient to observe the log of the volume rather than the volume itself. The log of sound power is measured in deciBels (dB). (You might be familiar with dB as a unit of sound volume). Hence, we shall consider the Log Spectrogram instead of just the spectrogram.\n","\n","Let us cmpute the Log Spectrogram of the audio file we read:"]},{"metadata":{"id":"589ETN681vbe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def log_specgram(audio, sample_rate, window_size=20, step_size=10, eps=1e-10):\n","    \n","    # Number of samples per window/segment\n","    nperseg = int(round(window_size * sample_rate / 1e3))\n","    \n","    # Number of overlapping samples\n","    noverlap = int(round(step_size * sample_rate / 1e3))\n","    \n","    # Compute the spectrogram\n","    freqs, times, spec = signal.spectrogram(audio,\n","                                            fs=sample_rate,\n","                                            window='hann',\n","                                            nperseg=nperseg,\n","                                            noverlap=noverlap,\n","                                            detrend=False)\n","    \n","    # Return log of the spectrogram values, with time axis in columns and frequencies in rows\n","    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0tWE7jL61vbg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["freqs, times, spectrogram = log_specgram(samples, sample_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FtLC5Bfr1vbm","colab_type":"text"},"cell_type":"markdown","source":["Let us plot the log spectrogram:"]},{"metadata":{"id":"jvgGsJZA1vbn","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["fig = plt.figure(figsize=(14, 4))\n","plt.imshow(spectrogram.T, aspect='auto', origin='lower', \n","           extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n","plt.gca().set_yticks(freqs[::16])\n","plt.gca().set_xticks(times[9::10])\n","plt.gca().set_title('Spectrogram of ' + filename)\n","plt.gca().set_ylabel('Frequency in Hz')\n","plt.gca().set_xlabel('Seconds')\n","plt.colorbar()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sRLCFO8Y1vbq","colab_type":"text"},"cell_type":"markdown","source":["As can be seen from the amplitude v/s time graph, the command is being uttered between 0.4 seconds and 0.6 seconds.\n","\n","As can be seen from the spectrogram, the command is composed more of lower frequencies than higher frequencies. The rest of the time when there is no speech, each frequency has an equal contribution to the sound. This is called \"White Noise\".\n","\n","Notice that the range of frequencies we are observing in the spectrogram is the linear scale between 0 Hz and 8000 Hz."]},{"metadata":{"id":"lm4K4O3B1vbr","colab_type":"text"},"cell_type":"markdown","source":["## 1.2. Mel Spectrogram\n","\n","Human ears tend to listen to sounds in the log scale. That means, at lower frequencies we can detect small changes, but at higher frequencies our ears become less sensitive to small changes. For example, the difference between 10 Hz and 20 Hz would sound almost the same to us as that between 1000 Hz and 2000 Hz. To observe this logarithmic change, the frequency scale is modified into the [\"mel frequency\" scale](https://en.wikipedia.org/wiki/Mel_scale).\n","\n","Let us compute the Mel Spectrogram using a convenient function in the `librosa` library in Python:"]},{"metadata":{"id":"JSmZ_i1l1vbs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# From this tutorial\n","# https://github.com/librosa/librosa/blob/master/examples/LibROSA%20demo.ipynb\n","S = librosa.feature.melspectrogram(samples, sr=sample_rate, hop_length=int(0.020*sample_rate), n_mels=128)\n","print(S.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r_KZ-HJj1vbv","colab_type":"text"},"cell_type":"markdown","source":["As mentioned before, the log of the spectrogram is a better parameter to observe rather than the spectrogram itself. Let us compute this using another convenient function in the `librosa` library:"]},{"metadata":{"id":"YzZWeBrD1vbw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Convert to log scale (dB). We'll use the peak power (max) as reference.\n","log_S = librosa.power_to_db(S, ref=np.max)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xk-YriDt1vb1","colab_type":"text"},"cell_type":"markdown","source":["Let's plot the log Mel spectrogam with the y-axis having frequenies in the mel scale instead of the linear scale: "]},{"metadata":{"id":"IuNmYVGF1vb3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.figure(figsize=(12, 4))\n","librosa.display.specshow(log_S, sr=sample_rate, x_axis='time', y_axis='mel')\n","plt.title('Log-power Mel spectrogram ')\n","plt.colorbar(format='%+02.0f dB')\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-cpSeZn-1vb4","colab_type":"text"},"cell_type":"markdown","source":["Observe that the frequencies in the y-axis are not linear in scale. "]},{"metadata":{"id":"U61Z8ib41vb5","colab_type":"text"},"cell_type":"markdown","source":["## 1.3 Mel Frequency Cepstral Coefficients (MFCCs)\n","\n","Next, \"Cepstral Coefficients\" are important numbers that describe speech information in audio. By computing these Cepstral Coefficients in the mel scale, we shall obtain Mel Frequency Cepstral Coefficients.\n","\n","For technical details, the procedure to compute MFCCs is:\n","\n","- Take the Discrete Fourier Transform on every sliding window over the audio with some overlap.\n","- Apply `n_mels` triangular Mel-scale filters onto the Fourier power spectrum, and apply logarithm to the outputs.\n","- Apply the Discrete Cosine Transform, and reduce dimensionality to `n_mfcc` dimensions.\n","\n","Let's use a convenient library function called `librosa.feature.mfcc` to compute MFCCs from the spectrogram:"]},{"metadata":{"id":"jY1PW7II1vb5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)\n","print(mfcc.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Omms34Yt1vb7","colab_type":"text"},"cell_type":"markdown","source":["Let us plot the MFCCs:"]},{"metadata":{"id":"Gh7HqNv61vb8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.figure(figsize=(12, 3))\n","librosa.display.specshow(mfcc)\n","plt.ylabel('MFCC coeffs')\n","plt.xlabel('Time')\n","plt.title('MFCC')\n","plt.colorbar()\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qzqltH471vcC","colab_type":"text"},"cell_type":"markdown","source":["## 1.4 Delta MFCCs\n","\n","MFCCs as such are quite powerful features, but even better features are their first-order and second-order derivatives.\n","\n","Let's use a convenient library function called `librosa.feature.mfcc` to compute the second-order delta MFCCs:"]},{"metadata":{"id":"Zsr_eURM1vcE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Find 1st order delta_mfcc\n","delta1_mfcc = librosa.feature.delta(mfcc, order=1)\n","\n","# Find 2nd order delta_mfcc\n","delta2_mfcc = librosa.feature.delta(mfcc, order=2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L2y94GRD1vcI","colab_type":"text"},"cell_type":"markdown","source":["Let's plot the 2nd order delta MFCCs:"]},{"metadata":{"id":"wYz7SkVy1vcI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.figure(figsize=(12, 6))\n","\n","plt.subplot(211)\n","librosa.display.specshow(delta1_mfcc)\n","plt.ylabel('1st order Delta MFCC coeffs')\n","plt.xlabel('Time')\n","plt.title('1st order Delta MFCC')\n","plt.colorbar()\n","plt.tight_layout()\n","\n","plt.subplot(212)\n","librosa.display.specshow(delta2_mfcc)\n","plt.ylabel('2nd order Delta MFCC coeffs')\n","plt.xlabel('Time')\n","plt.title('2nd order Delta MFCC')\n","plt.colorbar()\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5ZmSVr1k1vcN","colab_type":"text"},"cell_type":"markdown","source":["# 2. Load the Dataset\n","The dataset is of ~10GB in size and operating directly on it will take a lot of time, therefore we have included that as a Homework Exercise for those who are interested to go into that detail.\n","Our team has instead precomputed the features which can be loaded directly and computed on.\n","\n","# 3. Load MFCC features"]},{"metadata":{"id":"w5xp23EF1vcO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import scipy.io as sio\n","### Load MFCC Features\n","saved_vars = sio.loadmat('../Datasets/audio/mfcc_feats/tf_speech_mfcc_31st_jan18.mat')\n","# print(saved_vars.keys())\n","\n","mfcc_features_train = saved_vars['mfcc_features_train']\n","mfcc_labels_train = saved_vars['mfcc_labels_train']\n","\n","mfcc_features_val = saved_vars['mfcc_features_val']\n","mfcc_labels_val = saved_vars['mfcc_labels_val']\n","print(mfcc_features_train.shape,mfcc_features_val.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kt77z_tW1vcQ","colab_type":"text"},"cell_type":"markdown","source":["# 4. Load Deep Features"]},{"metadata":{"id":"haLEyDuR1vcQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["deep_features_train = np.squeeze(np.load('../Datasets/audio/deep_feats/train_set.npz'))\n","deep_labels_train = np.load('../Datasets/audio/deep_feats/train_labs.npz')\n","deep_features_val = np.squeeze(np.load('../Datasets/audio/deep_feats/validation_set.npz'))\n","deep_labels_val = np.load('../Datasets/audio/deep_feats/validation_labs.npz')\n","print(deep_features_train.shape, deep_features_val.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zlg5DyNv1vcZ","colab_type":"text"},"cell_type":"markdown","source":["# 5. Validation\n","\n","We want to choose the best model for speech classification among:\n","\n","- kNN classifier with MFCC features\n","\n","- kNN classifier with Deep features\n","\n","- Naive Bayes classifier with MFCC features\n","\n","- Naive Bayes classifier with Deep features\n","\n","- Multi-class Linear classifier with MFCC features\n","\n","- Multi-class Linear classifier with Deep features\n","\n","To do that, let's find the validation accuracies of each method using MFCC features and deep features.\n","\n","We shall use our familiar convenient function to choose an algorithm, train on training features and labels as inputs, and obtain accuracy on given features and labels."]},{"metadata":{"id":"5C3VYNbw1vca","colab_type":"text"},"cell_type":"markdown","source":["## Function to choose algorithm, features and labels to get accuracy\n","\n","We created a convenient function to which you can choose the algorithm you want to use - `'kNN'`, `'NB'` (Naive Bayes), or `'linear'` (multiclass linear classifier) - and input features and labels for training and validation/testing:\n","\n","**`acc = predict_and_find_accuracy(algorithm, train_features, train_labels, given_features, given_labels, k)`**\n","\n","INPUTS:\n","\n","- **`algorithm`** is one of `'kNN'`, `'NB'` (Naive Bayes), and `'linear'`,\n","\n","\n","- **`train_features`** are either `mfcc_features_train` or `deep_features_train`,\n","\n","\n","- **`train_labels`** are either `mfcc_labels_train` or `deep_labels_train`,\n","\n","\n","- **`given_features`** are `XXX_features_val` in case of validation, and `XXX_features_test` in case of testing (`XXX` being either `mfcc` or `deep` depending on what you chose to train the model on),\n","\n","\n","- **`given_labels`** are `XXX_features_val` in case of validation, and `XXX_features_test` in case of testing (`XXX` being either `mfcc` or `deep` depending on what you chose to train the model on),\n","\n","\n","- **`k`** is the value of $k$ to be used _in case_ we are using kNN classifier.\n","\n","OUTPUT:\n","\n","- **acc** is the accuracy of the `algorithm` on `given_features` & `given_labels` after being trained on `train_features` & `train_labels`."]},{"metadata":{"id":"aQD1Dkrb1vcb","colab_type":"text"},"cell_type":"markdown","source":["## 5.1. kNN Classifier\n","\n","We want to choose the value of $k$ based on validation accuracy. Choose $k$ among 1, 3, 7, 15, 51, 101."]},{"metadata":{"id":"ziBpWpbl1vcc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["values_of_k = [1, 3, 7, 15, 51, 101]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A7Xm5TKe1vce","colab_type":"text"},"cell_type":"markdown","source":["## 5.1.1. kNN with MFCC features"]},{"metadata":{"id":"cRZApQve1vcf","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 1: Find the best $k$ using validation accuracy on MFCC features**"]},{"metadata":{"id":"rdegBJLv1vcg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Your code here\n","best = 0\n","result = 0\n","for k in values_of_k:\n","    mfcc_kNN_acc = predict_and_find_accuracy('kNN', mfcc_features_train, mfcc_labels_train, mfcc_features_val, mfcc_labels_val, k)\n","    if best < mfcc_kNN_acc:\n","        best = mfcc_kNN_acc\n","        result = k\n","\n","best_k_mfcc = result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OQuqfurA1vct","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(best_k_mfcc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mQ7KQ8Wg1vcx","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 2: Find the validation accuracy of kNN classifier on MFCC features using best $k$**"]},{"metadata":{"id":"aiD8_Hxs1vcz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["kNN_mfcc_val_acc = predict_and_find_accuracy(\"kNN\", mfcc_features_train, mfcc_labels_train, mfcc_features_val, mfcc_labels_val, best_k_mfcc)\n","print(kNN_mfcc_val_acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ek7g11M71vdG","colab_type":"text"},"cell_type":"markdown","source":["## 5.1.2. kNN with Deep features"]},{"metadata":{"id":"eMz9n9oE1vdG","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 3: Find the best $k$ using validation accuracy on Deep features**"]},{"metadata":{"id":"JxMelPB31vdJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["deep_k_acc = []\n","\n","for k in values_of_k:\n","    deep_k_acc.append(predict_and_find_accuracy(\"kNN\", deep_features_train, deep_labels_train, deep_features_val, deep_labels_val, k))\n","    \n","best_k_deep = values_of_k[np.argmax(deep_k_acc)]\n","print(best_k_deep)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y8uPkZP01vdL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(best_k_deep)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J-P2MNh01vdR","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 4: Find the validation accuracy of kNN classifier on Deep features using best $k$**"]},{"metadata":{"id":"QPoH7e2S1vdT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["kNN_deep_val_acc = predict_and_find_accuracy(\"kNN\", deep_features_train, deep_labels_train, deep_features_val, deep_labels_val, best_k_deep)\n","print(kNN_deep_val_acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ekYiiRh61vda","colab_type":"text"},"cell_type":"markdown","source":["## 5.2 Naive Bayes classifier\n","\n","Find the validation accuracies with MFCC features and Deep features using Naive Bayes classifier."]},{"metadata":{"id":"Jfl9-_OC1vdb","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 5: Find the validation accuracy of Naive Bayes classifier on MFCC features**"]},{"metadata":{"id":"b1wtAAkJ1vdb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["NB_mfcc_val_acc = predict_and_find_accuracy(\"NB\", mfcc_features_train, mfcc_labels_train, mfcc_features_val, mfcc_labels_val)\n","print(NB_mfcc_val_acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q1Zrj4SO1vdd","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 6: Find the validation accuracy of Naive Bayes classifier on deep features**"]},{"metadata":{"id":"DRh__6Dg1vdd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["NB_deep_val_acc = predict_and_find_accuracy(\"NB\", deep_features_train, deep_labels_train, deep_features_val, deep_labels_val)\n","print(NB_deep_val_acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l6MbwqjB1vdf","colab_type":"text"},"cell_type":"markdown","source":["## 5.3 Multi-class linear classifier\n","\n","Find the validation accuracies with MFCC features and Deep features using Multi-class linear classifier."]},{"metadata":{"id":"_LAQS0Wn1vdg","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 7: Find the validation accuracy of Multi-class linear classifier on MFCC features**"]},{"metadata":{"id":"wo9CVu_g1vdg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["linear_mfcc_val_acc = predict_and_find_accuracy(\"linear\", mfcc_features_train, mfcc_labels_train, mfcc_features_val, mfcc_labels_val)\n","print(linear_mfcc_val_acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YLayIyU71vdi","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 8: Find the validation accuracy of Multi-class linear classifier on Deep features**"]},{"metadata":{"id":"dDYYVElz1vdj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["linear_deep_val_acc = predict_and_find_accuracy(\"linear\", deep_features_train, deep_labels_train, deep_features_val, deep_labels_val)\n","print(linear_deep_val_acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SlhTEF5q1vdl","colab_type":"text"},"cell_type":"markdown","source":["## 5.4 Choose best model\n","\n","Let us compare the validation accuracies of each model:"]},{"metadata":{"id":"_WKU-daw1vdl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(\"      Model                       Val acc \")\n","print(\"------------------------------------------\")\n","print(\"kNN with MFCC features         :   \", \"{0:.03f}\".format(kNN_mfcc_val_acc))\n","print(\"kNN with Deep features         :   \", \"{0:.03f}\".format(kNN_deep_val_acc))\n","print(\"Naive Bayes with MFCC features :   \", \"{0:.03f}\".format(NB_mfcc_val_acc))\n","print(\"Naive Bayes with Deep features :   \", \"{0:.03f}\".format(NB_deep_val_acc))\n","print(\"Linear with MFCC features      :   \", \"{0:.03f}\".format(linear_mfcc_val_acc))\n","print(\"Linear with Deep features      :   \", \"{0:.03f}\".format(linear_deep_val_acc))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K58XSY9E1vdn","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 9: Which is the best model among the above 6 based on their validation accuracies?**"]},{"metadata":{"id":"qSeF7F9M1vew","colab_type":"text"},"cell_type":"markdown","source":["# 6. Testing\n","\n","**Exercise 10: Find the testing accuracies.**"]},{"metadata":{"id":"ciaSVZGD1vex","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Finding Testing accuracies\n","NB_mfcc_test_acc = predict_and_find_accuracy(\"NB\", mfcc_features_train, mfcc_labels_train, mfcc_features_test, mfcc_labels_test)\n","NB_deep_test_acc = predict_and_find_accuracy(\"NB\", deep_features_train, deep_labels_train, deep_features_test, deep_labels_test)\n","kNN_mfcc_test_acc = predict_and_find_accuracy(\"kNN\", mfcc_features_train, mfcc_labels_train, mfcc_features_test, mfcc_labels_test)\n","kNN_deep_test_acc = predict_and_find_accuracy(\"kNN\", deep_features_train, deep_labels_train, deep_features_test, deep_labels_test)\n","linear_mfcc_test_acc = predict_and_find_accuracy(\"linear\", mfcc_features_train, mfcc_labels_train, mfcc_features_test, mfcc_labels_test)\n","linear_deep_test_acc = predict_and_find_accuracy(\"linear\", deep_features_train, deep_labels_train, deep_features_test, deep_labels_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f1XryB4U1ve1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(\"      Model                       Val acc      Test acc \")\n","print(\"--------------------------------------------------------\")\n","print(\"kNN with MFCC features         :   \", \"{0:.03f}\".format(kNN_mfcc_val_acc), \"      \", \"{0:.03f}\".format(kNN_mfcc_test_acc))\n","print(\"kNN with Deep features         :   \", \"{0:.03f}\".format(kNN_deep_val_acc), \"      \", \"{0:.03f}\".format(kNN_deep_test_acc))\n","print(\"Naive Bayes with MFCC features :   \", \"{0:.03f}\".format(NB_mfcc_val_acc), \"      \", \"{0:.03f}\".format(NB_mfcc_test_acc))\n","print(\"Naive Bayes with Deep features :   \", \"{0:.03f}\".format(NB_deep_val_acc), \"      \", \"{0:.03f}\".format(NB_deep_test_acc))\n","print(\"Linear with MFCC features      :   \", \"{0:.03f}\".format(linear_mfcc_val_acc), \"      \", \"{0:.03f}\".format(linear_mfcc_test_acc))\n","print(\"Linear with Deep features      :   \", \"{0:.03f}\".format(linear_deep_val_acc), \"      \", \"{0:.03f}\".format(linear_deep_test_acc))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dJFqz5hl1ve3","colab_type":"text"},"cell_type":"markdown","source":["**Do the validation accuracies estimate the testing accuracies well?**"]}]}